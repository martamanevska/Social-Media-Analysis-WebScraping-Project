{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39fd35c0-a8ed-4ee5-9384-9a4ca7880b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two main libraries\n",
    "from bs4 import BeautifulSoup       # to process html\n",
    "import requests\n",
    "import urllib.request,sys,time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import datetime\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0583c",
   "metadata": {},
   "source": [
    "## Why we scrape using api's instead of using selenium\n",
    "\n",
    "[That's the page were we scraped the links to the articles](https://www.reuters.com/site-search/?query=russia-ukraine+war)  \n",
    "[These are the api's that we found](https://www.reuters.com/pf/api/v3/content/fetch/articles-by-search-v2?query={%22keyword%22%3A%22russia-ukraine%20war%22%2C%22offset%22%3A0'%2C%22orderby%22%3A%22display_date%3Adesc%22%2C%22size%22%3A100%2C%22website%22%3A%22reuters%22}&d=138&_website=reuters)\n",
    "\n",
    "The problem that we had was that the page where we scraped the link to the article is client-side rendered, or at least the part with the links. That means that the server sends to the client the APIs that we found plus a script that renders in html the page. So we can't scrape that type of page only with requests and beautiful soup, the alternatives were to use selenium or go directly for the APIs. We choose the APIs because they are faster to scrape and also more reliable.\n",
    "\n",
    "In order to find the API's we have done a search on the page linked above, then looking at the network tab that came out when you analyze the source code of the page we have found the call to the server with the URL to the API's.\n",
    "\n",
    "![Screen of how we found the api's](reuters-api-screen.png \"Screen of how we found the api's\")\n",
    "\n",
    "In order to do this you need to perform a query on the search bar and then look at which response arrives from the server there is supposed to be one JSON reply which is the one with the data that we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0a6f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [04:04<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "#get all the links of the articles\n",
    "saved_links = []\n",
    "numberOfArticles = 0\n",
    "\n",
    "# scraping data directly from the api's of reuters\n",
    "for page in tqdm(range(0,4750,50)): # this cicle takes some minutes, consider to load the pickle file\n",
    "    url = 'https://www.reuters.com/pf/api/v3/content/fetch/articles-by-search-v2?query={%22keyword%22%3A%22russia-ukraine%20war%22%2C%22offset%22%3A'+str(numberOfArticles)+'%2C%22orderby%22%3A%22display_date%3Adesc%22%2C%22size%22%3A100%2C%22website%22%3A%22reuters%22}&d=138&_website=reuters'\n",
    "    result = requests.get(url).text\n",
    "    page += 50\n",
    "    \n",
    "    saved_links.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41adfda9",
   "metadata": {},
   "source": [
    "We save the links that we obtained because can happen to be blocked by the server, so is better to have a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8c543b-55a4-442c-bb93-3ac62455d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"reuters_links.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(saved_links, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7671ca93-5df0-48ef-9f2c-1520d9eaed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load\n",
    "open_file = open(\"reuters_links.pkl\", \"rb\")\n",
    "saved_links = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c394e",
   "metadata": {},
   "source": [
    "Since sometimes the reply by the server is an HTML page because there is an issue loading the JSON file we need to try to parse the JSON in order to see if it's a JSON or not and then remove the ones that are HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d38dbc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 175.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before:  95 \n",
      "Length after:  92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "before = len(saved_links)\n",
    "\n",
    "for i, link in tqdm(enumerate(saved_links)):\n",
    "    try:\n",
    "        json.loads(link)\n",
    "    except:\n",
    "        saved_links.remove(link)\n",
    "        \n",
    "print(\"Length before: \", before, \"\\nLength after: \", len(saved_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c820649",
   "metadata": {},
   "source": [
    "Here we save on the dataset all the information that we have gained from the APIs a part of the text that wasn't there, for the text we need to open each page and scrape it. We took all the data from the APIs because as we know pages can be different, so is better to use the information that we already have instead of trying to read them from the HTML. For the text we look into the article tag where the text of the article is supposed to be, then we perform a regex that searches for all the attributes named data-testid set to paragraph-x where the is a number. These are all the paragraph tags that have inside the text of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a7c87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [2:16:52, 89.27s/it] \n"
     ]
    }
   ],
   "source": [
    "#creating the dataframe\n",
    "\n",
    "ReutersArticles = pd.DataFrame({'title': pd.Series([], dtype='string'),\n",
    "                             'author': pd.Series([], dtype='string'),\n",
    "                             'date': pd.Series([], dtype='float'),\n",
    "                             'text': pd.Series([], dtype='string'),\n",
    "                             })\n",
    "\n",
    "\n",
    "for i, link in tqdm(enumerate(saved_links)):\n",
    "    try:\n",
    "        link = json.loads(link) # Since the api's gives back a json file we need to load as a json in order to access to the objects inside\n",
    "        iterations = len(link['result']['articles']) # we measure the number of article that we have for the next iteration\n",
    "\n",
    "        for j in range(0,iterations): # we have a object called result, with everything we are looking for inside\n",
    "            try:\n",
    "                page = requests.get('https://www.reuters.com' + link['result']['articles'][j]['canonical_url']) # from the canonical url one part is missing so we add that part, again the result is in the object result, article and then canonical_url. Since we have many articles we need to iterate over them.\n",
    "                soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "                # get article tag\n",
    "                article = soup.find_all(\"article\")[0]\n",
    "\n",
    "                #get title\n",
    "                title = link['result']['articles'][j]['title']\n",
    "\n",
    "                #get date\n",
    "                date = pd.to_datetime(link['result']['articles'][j]['published_time']).strftime('%d/%m/%y')\n",
    "\n",
    "                #get text\n",
    "                text = ''\n",
    "                for p in article.find_all('p', {'data-testid': re.compile(r'paragraph-\\d')}):\n",
    "                    text += p.text\n",
    "\n",
    "                # get author\n",
    "                try:\n",
    "                    author = ''\n",
    "                    for x in range(0,len(link['result']['articles'][j]['authors'])):\n",
    "                        author += link['result']['articles'][j]['authors'][x]['name']\n",
    "                except:\n",
    "                    author = None\n",
    "\n",
    "                ReutersArticles = ReutersArticles.append({\"title\": title, \"author\": author, \"date\": date, \"text\": text}, ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9ec7781-24a8-4b60-8823-9e390903237d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Homes smashed, 34 wounded in latest Russian st...</td>\n",
       "      <td>Ivan Lubysh-Kirdey</td>\n",
       "      <td>01/05/23</td>\n",
       "      <td>May 1 (Reuters) - Russia unleashed a fresh vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Factbox: Countries rush to evacuate foreign ci...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>25/04/23</td>\n",
       "      <td>KHARTOUM, April 25 (Reuters) - Countries have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Latest on the Ukraine war: Pope says Vatican i...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>22/04/23</td>\n",
       "      <td>April 22 (Reuters) - Russia unleashed a fresh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia's Prigozhin renews appeal for more ammu...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>01/05/23</td>\n",
       "      <td>May 1 (Reuters) - The head of the Wagner priva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ukraine to boycott World Judo Championships ov...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>01/05/23</td>\n",
       "      <td>May 1 (Reuters) - Ukrainian judokas will not t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9188</th>\n",
       "      <td>Russian parliament votes to tighten \"foreign a...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>07/06/22</td>\n",
       "      <td>LONDON, June 7 (Reuters) - The lower house of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9189</th>\n",
       "      <td>Russian attack destroys warehouses of major Uk...</td>\n",
       "      <td>Pavel Polityuk</td>\n",
       "      <td>07/06/22</td>\n",
       "      <td>KYIV, June 7 (Reuters) - Russian shelling dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9190</th>\n",
       "      <td>Russian parliament votes to break with Europea...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>07/06/22</td>\n",
       "      <td>LONDON, June 7 (Reuters) - Russia's parliament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9191</th>\n",
       "      <td>Russia says two Ukrainian ports ready to ship ...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>07/06/22</td>\n",
       "      <td>LONDON, June 7 (Reuters) - Russia said on Tues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9192</th>\n",
       "      <td>IBM winding down Russian operation, laying off...</td>\n",
       "      <td>Supantha Mukherjee</td>\n",
       "      <td>07/06/22</td>\n",
       "      <td>STOCKHOLM, June 7 (Reuters) - International Bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9193 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title              author  \\\n",
       "0     Homes smashed, 34 wounded in latest Russian st...  Ivan Lubysh-Kirdey   \n",
       "1     Factbox: Countries rush to evacuate foreign ci...             Reuters   \n",
       "2     Latest on the Ukraine war: Pope says Vatican i...             Reuters   \n",
       "3     Russia's Prigozhin renews appeal for more ammu...             Reuters   \n",
       "4     Ukraine to boycott World Judo Championships ov...             Reuters   \n",
       "...                                                 ...                 ...   \n",
       "9188  Russian parliament votes to tighten \"foreign a...             Reuters   \n",
       "9189  Russian attack destroys warehouses of major Uk...      Pavel Polityuk   \n",
       "9190  Russian parliament votes to break with Europea...             Reuters   \n",
       "9191  Russia says two Ukrainian ports ready to ship ...             Reuters   \n",
       "9192  IBM winding down Russian operation, laying off...  Supantha Mukherjee   \n",
       "\n",
       "          date                                               text  \n",
       "0     01/05/23  May 1 (Reuters) - Russia unleashed a fresh vol...  \n",
       "1     25/04/23  KHARTOUM, April 25 (Reuters) - Countries have ...  \n",
       "2     22/04/23  April 22 (Reuters) - Russia unleashed a fresh ...  \n",
       "3     01/05/23  May 1 (Reuters) - The head of the Wagner priva...  \n",
       "4     01/05/23  May 1 (Reuters) - Ukrainian judokas will not t...  \n",
       "...        ...                                                ...  \n",
       "9188  07/06/22  LONDON, June 7 (Reuters) - The lower house of ...  \n",
       "9189  07/06/22  KYIV, June 7 (Reuters) - Russian shelling dest...  \n",
       "9190  07/06/22  LONDON, June 7 (Reuters) - Russia's parliament...  \n",
       "9191  07/06/22  LONDON, June 7 (Reuters) - Russia said on Tues...  \n",
       "9192  07/06/22  STOCKHOLM, June 7 (Reuters) - International Bu...  \n",
       "\n",
       "[9193 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReutersArticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c117913b-0bc7-4691-bbd0-bc3efed8770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReutersArticles.to_csv(\"ReutersArticles.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
